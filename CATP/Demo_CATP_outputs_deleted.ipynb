{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nEEW_yUm07_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Necessary imports*"
      ],
      "metadata": {
        "id": "OLNzTQwqeoyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Reshape, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "starttime = time.time()\n",
        "# matplotlib.use('TkAgg')"
      ],
      "metadata": {
        "id": "RDs7jWGoeu_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIsWJhvchJG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy data generation\n",
        "We consider `n_sensors` sensors, each having data as a time series of sine wave with some uniform zero-mean noise added. Each sensors has a phase difference. The parameters are:\n",
        "\n",
        "* `n_sensors`: number of sensors in the dataset; 3 for this data\n",
        "*  `cycles`: How many time periods of sine wave in full data set; 300 for this data\n",
        "*  `resolution`: Total number of time stamps  in `cycles` time periods; 10K for this data\n",
        "* `phase`: phase difference between sensors; 9 for this data\n",
        "* `PERIODICITY`: computed as $$\\frac{\\text{resolution}}{\\text{cycles}}$$. This is also referred to as the daily offset in this demo script, since the periodicity is later used to compute the nearest neighbours in temporal bands at multiples of periodicity; (cf.`Figure 2`. in the paper)\n",
        "* `NOISE`: magnitude of noise in data; default 0.2\n"
      ],
      "metadata": {
        "id": "LLnwKrLterNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_sensors = 3\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"plots_from_demo_data\"):\n",
        "    os.mkdir(\"plots_from_demo_data\")\n",
        "\n",
        "def generate_continuous_dataset():\n",
        "    \"\"\"\n",
        "    Generates a continuous dataset based on sine wave cycles with added noise.\n",
        "\n",
        "    The function creates a dataset of sine waves for a specified number of cycles and resolution.\n",
        "    Noise is added to simulate real-world data. The dataset is plotted and saved as an image.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the generated dataset and the periodicity of the sine waves.\n",
        "    \"\"\"\n",
        "    cycles = 300 # how many sine cycles\n",
        "    resolution = 10000 # how many datapoints to generate\n",
        "    phase = 9\n",
        "\n",
        "    length = np.pi * 2 * cycles\n",
        "\n",
        "    sensor_list = []\n",
        "    NOISE_LEVEL = 0.2\n",
        "\n",
        "    for i in range(n_sensors):\n",
        "        values = np.sin( (np.arange(0, length, length / resolution)) - phase * i )\n",
        "        shape = values.shape\n",
        "        noise_with_mean_zero = (np.random.rand(*shape) - 0.5)\n",
        "\n",
        "        sensor_list.append ( values + noise_with_mean_zero * NOISE_LEVEL )\n",
        "\n",
        "    dataset = np.random.rand(len(sensor_list), resolution) * 0\n",
        "\n",
        "    for counter, sensor in enumerate(sensor_list):\n",
        "        dataset[counter, :] = sensor\n",
        "        plt.plot(sensor, label= \"sensor \" + str(counter+1))\n",
        "\n",
        "    PERIODICITY = int(resolution/cycles)\n",
        "    plt.plot(range(phase, PERIODICITY + phase), [1.5] * PERIODICITY, color=\"black\", linestyle=\"--\", label=\"daily offset \" + str (PERIODICITY))\n",
        "\n",
        "    plt.legend()\n",
        "    plt.ylim(-2, 2)\n",
        "    plt.xlim(0, 120)\n",
        "    plt.title(\"Raw data\")\n",
        "    plt.savefig(\"plots_from_demo_data/two_sensors_time_series.jpg\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "    return dataset, PERIODICITY\n",
        "large_dataset, PERIODICITY = generate_continuous_dataset()"
      ],
      "metadata": {
        "id": "TLJfMu0dgcRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to supervised labels:**\n",
        "The time series needs to converted to supervised labels in order to be modelled by the DL models. This is accomplished using the function `sample_blocks_for_XY`. The `i_o` value decides the number of time frames in input and ouput data. (cf. Figure 1 from the paper)\n"
      ],
      "metadata": {
        "id": "9vq2sYQohW4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(large_dataset, timestamp, i_o):\n",
        "    \"\"\"\n",
        "    Loads a subset of data from a larger dataset.\n",
        "    The dataloader should be able to access the neighbours using a timestamp index.\n",
        "\n",
        "    Args:\n",
        "        large_dataset (np.ndarray): The larger dataset from which to load data.\n",
        "        i (int): The starting index for loading data.\n",
        "        i_o (int): The size of the input-output blocks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the X and Y blocks for the specified index.\n",
        "\n",
        "    Refer to: https://github.com/mie-lab/Complexity-Aware-Traffic-Prediction/blob/1c7c302e68276a5c8a61be7bbefca5d36b871ec6/CATP/preprocessing/ProcessRaw.py#L261C2-L261C2\n",
        "    for implementation for traffic-4-cast dataset\n",
        "    \"\"\"\n",
        "    x_i = large_dataset[:,timestamp: timestamp+i_o]\n",
        "    y_i = large_dataset[:,timestamp + i_o + 1: timestamp + 2 * i_o + 1]\n",
        "    return x_i, y_i\n",
        "\n",
        "def sample_blocks_for_XY(dataset, i_o):\n",
        "    \"\"\"\n",
        "    Converts sequential data into X,Y for time series supervised regression task\n",
        "    This function ensures that there is enough room in the dataset for the input-output blocks.\n",
        "\n",
        "    Args:\n",
        "        dataset (np.ndarray): The dataset from which to sample.\n",
        "        i_o (int): The size of the input-output blocks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing arrays for X, Y, and a list of indices.\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    max_start_index = dataset.shape[1] - 2 * i_o - 1 # Ensure room for i_o at the ends of dataset\n",
        "    indices_list = []\n",
        "    for i in range(max_start_index):\n",
        "        x_block, y_block = dataloader(dataset, timestamp=i, i_o=i_o) # dataset[:, start_index:start_index + i_o]\n",
        "        X.append(x_block)\n",
        "        Y.append(y_block)\n",
        "        indices_list.append(i)\n",
        "    return np.array(X), np.array(Y), indices_list"
      ],
      "metadata": {
        "id": "PF7078Pwh7fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build model**\n",
        "We use keras with tensorflow backend, the input and output shapes are shown for references"
      ],
      "metadata": {
        "id": "RQFjlJ7jh8UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_fc(i_o):\n",
        "    # Calculate the total number of elements in the input (e.g., 2*100 for a 2x100 input)\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=((n_sensors, i_o))),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(n_sensors * i_o),  # Output layer with as many neurons as the total elements in the input\n",
        "        Reshape((n_sensors, i_o))  # Reshape the output to match the input shape\n",
        "    ])\n",
        "    model.compile(optimizer='sgd', loss='mse')\n",
        "    return model\n",
        "\n",
        "def linf_distance(a,b):\n",
        "    \"\"\"\n",
        "    Computes the L-infinity distance between two vectors.\n",
        "    Args:\n",
        "        a (np.ndarray): The first array.\n",
        "        b (np.ndarray): The second array.\n",
        "\n",
        "    Returns:\n",
        "        float: The L-infinity distance between the two arrays.\n",
        "    \"\"\"\n",
        "    return np.max(np.abs(a.flatten()-b.flatten()))\n",
        "\n",
        "dummy_model = build_model_fc(i_o = 10)\n",
        "dummy_model.summary()"
      ],
      "metadata": {
        "id": "xj_ivC1NimRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model complexity function\n",
        "\n",
        "To make the model complexity computation efficient (and straightforward to implement), the key implementation steps are shown below:\n",
        "\n",
        "\n",
        "\n",
        "* **Custom Dataloader**:  For each input data point, find its temporal neighbours; this is a constant time operations for time series tasks, since the dataloader can be tweaked to return the input data point at a given `timestamp`."
      ],
      "metadata": {
        "id": "F48rIOEvjAcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dataloader(large_dataset, timestamp, i_o):\n",
        "    \"\"\"\n",
        "    Loads a subset of data from a larger dataset.\n",
        "    The dataloader should be able to access the neighbours using a timestamp index.\n",
        "\n",
        "    Args:\n",
        "        large_dataset (np.ndarray): The larger dataset from which to load data.\n",
        "        i (int): The starting index for loading data.\n",
        "        i_o (int): The size of the input-output blocks.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the X and Y blocks for the specified index.\n",
        "\n",
        "    Refer to: https://github.com/mie-lab/Complexity-Aware-Traffic-Prediction/blob/1c7c302e68276a5c8a61be7bbefca5d36b871ec6/CATP/preprocessing/ProcessRaw.py#L261C2-L261C2\n",
        "    for implementation for traffic-4-cast dataset\n",
        "    \"\"\"\n",
        "    x_i = large_dataset[:,timestamp: timestamp+i_o]\n",
        "    y_i = large_dataset[:,timestamp + i_o + 1: timestamp + 2 * i_o + 1]\n",
        "    return x_i, y_i"
      ],
      "metadata": {
        "id": "EmiTgrrmkJwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Clubbing all `model.predict` together**: To make the Model complexity ($MC$) efficient, it is recommended to compute all predictions using a dataloader since it is inefficient to call `model.predict` intermittently while doing other processing for neighbourhood search etc.. Once all predictions are ready, we can compute the model complexity by measuring the degree to which the model transforms the input space. As excerpted from the `model_complexity_MC` function below, since here our data set is small, all predictions can be stored in a list. For real datasets, (as was the case in our experiments in the paper), all predictions can be saved to disk and a similar dataloader can be used to extract the relevant predictions later on.\n",
        "    ```python\n",
        "    # predict for all data points so that we can process later\n",
        "    predicted = [0] * N\n",
        "    for i in tqdm(range(0, N, batch_size), \"Predicting for all data points\"):\n",
        "        X = []\n",
        "        for j in range(batch_size):\n",
        "            x,y = dataloader(large_dataset, j, i_o=i_o)\n",
        "            X.append(x)\n",
        "        X = np.array(X)\n",
        "        predicted[i:i+X.shape[0]] = [predicted for predicted in model_predict(X.reshape((-1, n_sensors, i_o)))]\n",
        "\n",
        "    ```\n",
        "\n",
        "* **Determine the neighbours**: The neighbours in temporal band of look forward and backward policy of `n_h` at multiples of periodicity are searched by:\n",
        "    ```python\n",
        "    for day in range(-n_d, n_d+1):\n",
        "        for hour in range(-n_h, n_h+1):\n",
        "            j = i + day * periodicity + hour\n",
        "    ```\n",
        "From Equation 4 in the paper, we had the set of neighbours for data tensor at time stamp `t` as  \\mathbb{U}_t, given by:\n",
        "$$\n",
        " \\mathbb{U}_t = \\left\\{ t' \\: | \\: t' \\neq t, \\: t' =~t~\\pm \\underbrace{n_d \\cdot 24 \\cdot \\frac{60}{p}}_{\\text{daily periodicity offset}} \\pm \\underbrace{~~~n_h~~~}_{h \\text{ hours look forward & backward}} \\right\\}\n",
        "$$\n",
        "Given our custom dataloader, the input data point at timestamp `j` values can be extracted in constant time as:\n",
        "```python\n",
        "x_j, y_j = dataloader(large_dataset, j, i_o=i_o)\n",
        "```\n",
        "\n",
        "* **Compute the maximum distance in input space**\n",
        "From equations 6 and 7 in the paper, we have:\n",
        "\\begin{equation}\n",
        "\\mathbb{T}_\\mathbf{x} = \\{\\mathbf{x_t} \\: | \\: t \\in \\mathbb{U}_t\\}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    r_\\mathbf{x}=\\text{max}(\\{||\\mathbf{x_i}-\\mathbf{x}||_{\\infty}\\}\\; | \\mathbf{x_i} \\in \\mathbb{T}_\\mathbf{x})\n",
        "\\end{equation}\n",
        "\n",
        "In the function `model_complexity_MC`, the $r_x$ is computed using:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        ".\n",
        ".   # inside the loop\n",
        "    neighbour_index_list.append(j)\n",
        "    x_distance_list.append(linf_distance(x_i, x_j))\n",
        "max_dist_x = np.max(x_distance_list)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "* **Criticality of each data point**:\n",
        "For each data point, we track the predictions (output of `model.predict`, which has been pre-computed and saved in the variable `predictions`), to compute the criticality defined in Equation 8 as:\n",
        "\\begin{equation}\n",
        "CRIT(\\mathbf{x}|f,\\mathcal{D}) = \\sum_{\\mathbf{x_j} \\in \\mathbb{T}_\\mathbf{x}} \\left(d_{f(\\mathbf{x_j})} - r_\\mathbf{x}\\right) \\cdot 1_{d_{f(\\mathbf{x_j})}>r_\\mathbf{x}}\n",
        "\\end{equation}\n",
        "```python\n",
        "        # inside loop\n",
        "        compute_criticality = [0]\n",
        "        for y_distance in y_distance_list:\n",
        "            if y_distance > max_dist_x:\n",
        "                compute_criticality.append(y_distance)\n",
        "        criticality = sum(compute_criticality)\n",
        "        list_of_criticality_values.append(criticality)\n",
        "```\n",
        "\n",
        "* **$MC$ as the mean over all $n$ criticality values**:\n",
        "\n",
        "```python\n",
        "# outside loop\n",
        "return np.mean(list_of_criticality_values) # computed complexity value of the model\n",
        "```\n",
        "Reproducing the Equation 9 from the paper, we have:\n",
        "\\begin{equation}\n",
        "MC(f| \\mathcal{D}) = \\frac{1}{N}\\sum_{k=1}^{N} CRIT(\\mathbf{x}_k|f, \\mathcal{D})\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "nAqzIZtBkQHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_complexity_MC(large_dataset,\n",
        "                i_o,\n",
        "                n,\n",
        "                model_predict,\n",
        "                periodicity,\n",
        "                n_d=3,\n",
        "                n_h=2,\n",
        "                batch_size=32):\n",
        "    \"\"\"\n",
        "    Computes the model complexity metric using a given DL model\n",
        "\n",
        "    This function is similar to `model_complexity_MC` but uses the ground truth data as the prediction\n",
        "    from the perfect model. It calculates the intrinsic complexity based on input-output distances.\n",
        "\n",
        "    Args:\n",
        "        large_dataset (np.ndarray): The dataset to compute the complexity on.\n",
        "        i_o (int): The number of frames in input and output.\n",
        "        n (int): The number of data points to consider in the complexity calculation.\n",
        "        periodicity refers to the offset required for 1 day;\n",
        "        n_d=3 corresponds to 1 week of neighbours (3 days look ahead and back; and the current day)\n",
        "        n_h=2 corresponds to 2 hours of neighbours (1 hour look ahead and back)\n",
        "        model_predict (function): The prediction function of the model\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    list_of_criticality_values = []\n",
        "\n",
        "    N =  large_dataset.shape[1]\n",
        "\n",
        "    # predict for all data points so that we can process later\n",
        "    predicted = [0] * N\n",
        "    for i in tqdm(range(0, N, batch_size), \"Predicting for all data points\"):\n",
        "        X = []\n",
        "        for j in range(batch_size):\n",
        "            x,y = dataloader(large_dataset, j, i_o=i_o)\n",
        "            X.append(x)\n",
        "        X = np.array(X)\n",
        "\n",
        "        predicted[i:i+X.shape[0]] = [predicted for predicted in model_predict(X.reshape((-1, n_sensors, i_o)))]\n",
        "\n",
        "\n",
        "    for i in tqdm(range(i_o, n), \"Iterating over all \" + str(n) + \" data points\"):\n",
        "\n",
        "        # create list of all neighbours in temporal bands\n",
        "        # at multiples of periodicity\n",
        "        neighbour_list = []\n",
        "        x_distance_list = []\n",
        "        neighbour_index_list = []\n",
        "        x_i, y_i = dataloader(large_dataset, i, i_o=i_o)\n",
        "\n",
        "        f_x_i = predicted[i]             # f(x_i)\n",
        "\n",
        "        for day in range(-n_d, n_d+1):\n",
        "            for hour in range(-n_h, n_h+1):\n",
        "\n",
        "                j = i + day * periodicity + hour\n",
        "\n",
        "                # ignore the tensors which are at the boundaries of the dataset\n",
        "                if j + i_o < 0 or j < 0 or \\\n",
        "                        j >= large_dataset.shape[1] or j+i_o >= large_dataset.shape[1]:\n",
        "                    continue\n",
        "\n",
        "                if j != i:\n",
        "                    x_j, y_j = dataloader(large_dataset, j, i_o=i_o)\n",
        "                    assert (x_j.shape == x_i.shape)\n",
        "                    neighbour_list.append(x_j)\n",
        "                    neighbour_index_list.append(j)\n",
        "                    x_distance_list.append(linf_distance(x_i, x_j))\n",
        "\n",
        "        max_dist_x = np.max(x_distance_list)\n",
        "\n",
        "        y_distance_list = []\n",
        "        for neighbour_index in neighbour_index_list:\n",
        "            # get f(x)\n",
        "            f_x_j = predicted[neighbour_index] # model_predict(neighbour.reshape((-1,2,i_o)))\n",
        "            y_distance_list.append(linf_distance(f_x_j, f_x_i))\n",
        "\n",
        "        compute_criticality = [0]\n",
        "        for y_distance in y_distance_list:\n",
        "            if y_distance > max_dist_x:\n",
        "                compute_criticality.append(y_distance)\n",
        "\n",
        "        criticality = sum(compute_criticality)\n",
        "\n",
        "        list_of_criticality_values.append(criticality)\n",
        "    return np.mean(list_of_criticality_values)\n"
      ],
      "metadata": {
        "id": "ozeyQG5nsG2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The Intrinsic complexity ($IC$)** is implemented similarly as $MC$ with the only difference that $f_{PM} (x_j) = y_j$ instead of $f(x_j)$ = `model.predict`($x_j$). In the following function, this is marked as:         \n",
        "`# THE ONLY DIFFERENCE FROM MODEL COMPLEXITY COMPUTATION`\n"
      ],
      "metadata": {
        "id": "oZwdc3LQsH_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def intrinsic_complexity_IC(large_dataset,\n",
        "                i_o,\n",
        "                n,\n",
        "                model_predict,\n",
        "                periodicity,\n",
        "                n_d=3,\n",
        "                n_h=2,\n",
        "                batch_size=32):\n",
        "    \"\"\"\n",
        "    Computes the intrinsic complexity metric using ground truth data.\n",
        "\n",
        "    This function is similar to `model_complexity_MC` but uses the ground truth data as the prediction\n",
        "    from the perfect model. It calculates the intrinsic complexity based on input-output distances.\n",
        "\n",
        "    Args:\n",
        "        large_dataset (np.ndarray): The dataset to compute the complexity on.\n",
        "        i_o (int): The number of frames in input and output.\n",
        "        n (int): The number of data points to consider in the complexity calculation.\n",
        "        periodicity refers to the offset required for 1 day;\n",
        "        n_d=3 corresponds to 1 week of neighbours (3 days look ahead and back; and the current day)\n",
        "        n_h=2 corresponds to 2 hours of neighbours (1 hour look ahead and back)\n",
        "        model_predict (function): The prediction function of the model\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    list_of_criticality_values = []\n",
        "\n",
        "    N =  large_dataset.shape[1]\n",
        "\n",
        "    # predict for all data points so that we can process later\n",
        "\n",
        "    predicted = [0] * N\n",
        "    for i in tqdm(range(0, N, batch_size), \"Predicting for all data points\"):\n",
        "        X = []\n",
        "        Y = []  # only differences from MC function. Here we use the ground truth as the prediction\n",
        "                # from the perfect model\n",
        "        for j in range(batch_size):\n",
        "            x,y = dataloader(large_dataset, j, i_o=i_o)\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "        X = np.array(X)\n",
        "\n",
        "\n",
        "        # THE ONLY DIFFERENCE FROM MODEL COMPLEXITY COMPUTATION\n",
        "        predicted[i:i+X.shape[0]] = [predicted for predicted in Y]\n",
        "\n",
        "\n",
        "\n",
        "    for i in tqdm(range(i_o, n), \"Iterating over all \" + str(n) + \" data points\"):\n",
        "\n",
        "        # create list of all neighbours in temporal bands\n",
        "        # at multiples of periodicity\n",
        "        neighbour_list = []\n",
        "        x_distance_list = []\n",
        "        neighbour_index_list = []\n",
        "        x_i, y_i = dataloader(large_dataset, i, i_o=i_o)\n",
        "\n",
        "        f_x_i = predicted[i]   # f(x_i)\n",
        "\n",
        "        for day in range(-n_d, n_d+1):\n",
        "            for hour in range(-n_h, n_h+1):\n",
        "\n",
        "                j = i + day * periodicity + hour\n",
        "\n",
        "                # ignore the tensors which are at the boundaries of the dataset\n",
        "                if j + i_o < 0 or j < 0 or \\\n",
        "                        j >= large_dataset.shape[1] or j+i_o >= large_dataset.shape[1]:\n",
        "                    continue\n",
        "\n",
        "                if j != i:\n",
        "                    x_j, y_j = dataloader(large_dataset, j, i_o=i_o)\n",
        "                    assert (x_j.shape == x_i.shape)\n",
        "                    neighbour_list.append(x_j)\n",
        "                    neighbour_index_list.append(j)\n",
        "                    x_distance_list.append(linf_distance(x_i, x_j))\n",
        "\n",
        "        max_dist_x = np.max(x_distance_list)\n",
        "\n",
        "        y_distance_list = []\n",
        "        for neighbour_index in neighbour_index_list:\n",
        "            f_x_j = predicted[neighbour_index] # model_predict(neighbour.reshape((-1,2,i_o)))\n",
        "            y_distance_list.append(linf_distance(f_x_j, f_x_i))\n",
        "\n",
        "        compute_criticality = [0]\n",
        "        for y_distance in y_distance_list:\n",
        "            if y_distance > max_dist_x:\n",
        "                compute_criticality.append(y_distance)\n",
        "\n",
        "        criticality = sum(compute_criticality)\n",
        "\n",
        "        list_of_criticality_values.append(criticality)\n",
        "    return np.mean(list_of_criticality_values) # computed complexity value of the model\n"
      ],
      "metadata": {
        "id": "3LsI_jJds3EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Vanilla codes for plotting training and visualising predictions of time series.`"
      ],
      "metadata": {
        "id": "4CXSoEiNs3uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_training_curves(model_identifer, history):\n",
        "    \"\"\"\n",
        "    plotting the training and validation loss with time\n",
        "    \"\"\"\n",
        "    loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(epochs, loss, 'tab:blue', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'tab:orange', label='Validation loss')\n",
        "    plt.title('Training Loss' + model_identifer)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(\"plots_from_demo_data/training_curve_\"+ model_identifer +\".jpg\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "def plot_selected_predictions_val_data(val_data, i_o, model_predict, model_identifier):\n",
        "    \"\"\"\n",
        "    plotting selected data points from the validation data to show the prediction performance\n",
        "    \"\"\"\n",
        "    indices = [0, 50, 1400]\n",
        "    ax = [0] * 3\n",
        "    plt.clf()\n",
        "    fig, (ax[0], ax[1], ax[2]) = plt.subplots(3)\n",
        "\n",
        "    for counter, i in enumerate (indices):\n",
        "        x,y = dataloader(val_data, timestamp=i, i_o=i_o)\n",
        "        y_predict = model_predict(x.reshape((-1, n_sensors, i_o)))\n",
        "        ax[counter].plot(x[0, :].flatten().tolist() + y[0, :].flatten().tolist(), label=\"sensor 1 GT\", color=\"tab:blue\",\n",
        "                         linewidth=2)\n",
        "        ax[counter].plot(x[1, :].flatten().tolist() + y[1, :].flatten().tolist(), label=\"sensor 2 GT\", color=\"tab:orange\",\n",
        "                         linewidth=2)\n",
        "\n",
        "        ax[counter].plot(x[0, :].flatten().tolist() + y_predict[0, 0, :].flatten().tolist(), label=\"sensor 1 pred\", color=\"tab:blue\",\n",
        "                         linewidth=0.6)\n",
        "        ax[counter].plot(x[1, :].flatten().tolist() + y_predict[0, 1, :].flatten().tolist(), label=\"sensor 2 pred\", color=\"tab:orange\",\n",
        "                         linewidth=0.6)\n",
        "        ax[counter].set_title(model_identifier)\n",
        "\n",
        "    plt.legend(fontsize=6, loc=\"upper left\")\n",
        "    # plt.title(model_identifier)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots_from_demo_data/Predictions_\" + model_identifier + \".jpg\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.clf()"
      ],
      "metadata": {
        "id": "yParFJWatWRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wjpUC8mekMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Driver function (\"`__main__`\")"
      ],
      "metadata": {
        "id": "gx5nqUxOtXrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "large_dataset, PERIODICITY = generate_continuous_dataset()\n",
        "print (large_dataset.shape)\n",
        "\n",
        "i_o = 7  # Length of Input and output sequences\n",
        "EPOCH = 20\n",
        "n_for_complexity_calculation = 5000\n",
        "\n",
        "TrainX, TrainY, indices_list = sample_blocks_for_XY(large_dataset[:, :-2000], i_o)\n",
        "ValX, ValY,_ = sample_blocks_for_XY(large_dataset[:,-2000:], i_o)\n",
        "\n",
        "\n",
        "\n",
        "model_fc = build_model_fc(i_o=i_o)\n",
        "model_fc.summary()\n",
        "\n",
        "history = model_fc.fit(TrainX, TrainY, epochs=EPOCH, verbose=2, validation_data=\n",
        "                                        [ValX, ValY], batch_size=32)\n",
        "plot_training_curves(model_identifer=\"_fc_\", history=history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "metric_value_IC = intrinsic_complexity_IC(large_dataset[:, :8000],\n",
        "                                   i_o,\n",
        "                                   n_for_complexity_calculation,\n",
        "                                   model_fc.predict,\n",
        "                                   periodicity=PERIODICITY,\n",
        "                                   n_d=2,\n",
        "                                   n_h=20,\n",
        "                                   batch_size=32\n",
        "                                   )\n",
        "\n",
        "\n",
        "# Compute the custom metric for one example\n",
        "metric_value_fc = model_complexity_MC(large_dataset[:, :8000],\n",
        "                                   i_o,\n",
        "                                   n_for_complexity_calculation,\n",
        "                                   model_fc.predict,\n",
        "                                   periodicity=PERIODICITY,\n",
        "                                   n_d=2,\n",
        "                                   n_h=20,\n",
        "                                   batch_size=32\n",
        "                                   )\n",
        "\n",
        "\n",
        "\n",
        "predicted_val_Y = model_fc.predict(ValX)\n",
        "assert (predicted_val_Y.shape == ValY.shape)\n",
        "print (\"MSE: (FC) \",  np.mean( (ValY - predicted_val_Y) ** 2 ))\n",
        "\n",
        "plot_selected_predictions_val_data(large_dataset[:, -2000:], i_o=i_o, model_predict=model_fc.predict, model_identifier=\"_FC_\")\n"
      ],
      "metadata": {
        "id": "DeAU6-H1th-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Intrinsic Complexity: \", metric_value_IC)\n",
        "print(\"Model complexity Fully Connected: \", metric_value_fc)\n",
        "print (\"End-to-end Run time of script: \", round(time.time() - starttime, 2))"
      ],
      "metadata": {
        "id": "5-XZO64SwJzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHUPPwwVekyf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AQi1mM2THSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSASh3rNRLTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKkQ3wfkm2e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Same thing for LSTM model"
      ],
      "metadata": {
        "id": "Te_yOY0FtnoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_lstm(i_o):\n",
        "    # Calculate the total number of elements in the input (e.g., 2*100 for a 2x100 input)\n",
        "    output_shape = (n_sensors, i_o)  # Adjust this to your desired output shape\n",
        "    total_output_elements = np.prod(output_shape)\n",
        "\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(n_sensors, i_o), return_sequences=False),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(total_output_elements),  # Ensure this matches the total number of elements in the output shape\n",
        "        Reshape(output_shape)  # Reshape the output to the desired shape\n",
        "    ])\n",
        "    model.compile(optimizer='sgd', loss='mse')\n",
        "    return model\n",
        "\n",
        "model_lstm = build_model_lstm(i_o=i_o)\n",
        "model_lstm.summary()\n",
        "history = model_lstm.fit(TrainX, TrainY, epochs=EPOCH, verbose=2, validation_data=\n",
        "                                        [ValX, ValY], batch_size=32)\n",
        "plot_training_curves(model_identifer=\"_lstm_\", history=history)\n",
        "\n",
        "metric_value_lstm = model_complexity_MC(large_dataset[:, :8000],\n",
        "                                   i_o,\n",
        "                                   5000,\n",
        "                                   model_lstm.predict,\n",
        "                                   periodicity=PERIODICITY,\n",
        "                                   n_d=2,\n",
        "                                   n_h=20,\n",
        "                                   batch_size = 32\n",
        "                                    )\n",
        "predicted_val_Y = model_lstm.predict(ValX)\n",
        "assert (predicted_val_Y.shape == ValY.shape)\n",
        "print (\"MSE: (LSTM) \",  np.mean( (ValY - predicted_val_Y) ** 2 ))\n",
        "plot_selected_predictions_val_data(large_dataset[:, -2000:], i_o=i_o, model_predict=model_lstm.predict, model_identifier=\"_LSTM_\")\n",
        "print(\"Model complexity LSTM: \", metric_value_lstm)"
      ],
      "metadata": {
        "id": "apBPCyY5tq0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlMSdplUvj8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
