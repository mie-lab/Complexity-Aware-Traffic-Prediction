{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import numpy as np\n",
    "from smartprint import smartprint as sprint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b0bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914954f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape, LeakyReLU, Input\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, losses, optimizers, models, metrics\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from smartprint import smartprint as sprint\n",
    "from scipy.spatial import minkowski_distance_p\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# compute CSR \n",
    "def compute_criticality_smooth(model_predict, training_data_folder, y_thresh=30, tnl=8, N=1000, \\\n",
    "                                  case=\"default\", PM=True):\n",
    "    \"\"\"\n",
    "    tnl: temporal_neighbour_limit \n",
    "    model_predict = model.predict    \n",
    "    case: [\"mean\", \"default\", \"max\", \"ratios\", \"min\", \"fractional\"]\n",
    "    PM: Perfect model\n",
    "    \"\"\"\n",
    "    assert case in [\"mean\", \"default\", \"max\", \"ratios\", \"min\", \"fractional\"]\n",
    "    count_CS = []\n",
    "    cache_dict = {} # to reduce the number of file reads \n",
    "    hit, miss = 0, 0 \n",
    "    filenames = glob.glob(training_data_folder + \"/*.npy\")\n",
    "    filenames = [f for f in filenames if \"_x\" in f]\n",
    "    \n",
    "    range_of_fnames = list(range(tnl+1, len(filenames)-tnl))\n",
    "    random.shuffle(range_of_fnames)\n",
    "    \n",
    "    \n",
    "    total_count = 0 \n",
    "    for i in (range_of_fnames[:N]): #, desc=\"computing CSR for training dataset\"):\n",
    "        total_count += 1 \n",
    "        \n",
    "        file = filenames[i]\n",
    "        n = str(i) \n",
    "        if int(n) in cache_dict:\n",
    "            x,y = cache_dict[int(n)]\n",
    "            hit += 1 \n",
    "        else:\n",
    "            x = np.load(training_data_folder + \"/\"+ n +\"_x\" + \".npy\") \n",
    "            y = np.load(training_data_folder + \"/\"+ n +\"_y\" + \".npy\") \n",
    "            cache_dict[int(n)] = x,y\n",
    "            miss += 1 \n",
    "\n",
    "        neighbours_x = []        \n",
    "        neighbours_y = []\n",
    "\n",
    "        for j in range(-tnl, tnl):\n",
    "            neigh = int(n) + j\n",
    "            if j==0:\n",
    "                continue\n",
    "            if neigh in cache_dict:\n",
    "                x_hat, y_hat = cache_dict[neigh]\n",
    "                hit += 1\n",
    "            else:\n",
    "                x_hat = np.load(training_data_folder + \"/\"+ str(neigh) + \"_x\" + \".npy\") \n",
    "                y_hat = np.load(training_data_folder + \"/\"+ str(neigh) + \"_y\" + \".npy\")\n",
    "                cache_dict[neigh] = x_hat, y_hat\n",
    "                miss += 1 \n",
    "                \n",
    "            x_hat = x_hat.reshape((-1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "            x_hat = np.moveaxis(x_hat, [0, 1, 2, 3], [0, 2, 3, 1])[..., np.newaxis]\n",
    "            \n",
    "            y_hat = y_hat.reshape((-1, y.shape[0], y.shape[1], y.shape[2]))\n",
    "            y_hat = np.moveaxis(y_hat, [0, 1, 2, 3], [0, 2, 3, 1])[..., np.newaxis]\n",
    "\n",
    "            neighbours_x.append(x_hat)            \n",
    "            neighbours_y.append(y_hat)\n",
    "        \n",
    "        if PM:\n",
    "            prediction = (np.vstack(tuple(neighbours_y)))\n",
    "        else: \n",
    "            prediction = model_predict(np.vstack(tuple(neighbours_x)))\n",
    "            \n",
    "        x = x.reshape((-1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "        x = np.moveaxis(x, [0, 1, 2, 3], [0, 2, 3, 1])[..., np.newaxis]        \n",
    "        y = y.reshape((-1, y.shape[0], y.shape[1], y.shape[2]))\n",
    "        y = np.moveaxis(y, [0, 1, 2, 3], [0, 2, 3, 1])[..., np.newaxis]\n",
    "        \n",
    "        a,b,c,d = prediction.shape[1], prediction.shape[2], prediction.shape[3], prediction.shape[4]\n",
    "        prediction = prediction.reshape((-1, a*b*c*d))\n",
    "        y = y.reshape((-1, a*b*c*d))\n",
    "\n",
    "        dist = minkowski_distance_p(prediction, y, np.inf)\n",
    "\n",
    "#         sprint (prediction.shape, y.shape, x.shape, x_hat.shape, y_hat.shape)\n",
    "\n",
    "        if case == \"mean\":\n",
    "            count_CS.append(np.mean(dist))\n",
    "        elif case == \"default\":\n",
    "            if np.any(dist > y_thresh):            \n",
    "                count_CS.append(x)\n",
    "        elif case == \"fractional\":\n",
    "            count_CS.append((dist>y_thresh).sum()/dist.shape[0])                \n",
    "        elif case == \"max\":\n",
    "            count_CS.append(np.max(dist))\n",
    "        elif case == \"min\":\n",
    "            count_CS.append(np.min(dist))\n",
    "        elif case == \"ratios\":\n",
    "\n",
    "            x_neighbours = (np.vstack(tuple(neighbours_x)))\n",
    "#             sprint (x.shape, x_neighbours.shape, prediction.shape)\n",
    "            a,b,c,d = x.shape[1], x.shape[2], x.shape[3], x.shape[4]\n",
    "            x_neighbours = x_neighbours.reshape((-1, a*b*c*d))\n",
    "            x = x.reshape((-1, a*b*c*d))\n",
    "\n",
    "            distX = minkowski_distance_p(x_neighbours, x, np.inf) \n",
    "#             from IPython.core.debugger import Pdb; Pdb().set_trace()            \n",
    "            ratio = dist/distX\n",
    "            sprint (ratio)\n",
    "            count_CS.append((ratio>1).sum()/dist.shape[0])\n",
    "            \n",
    "        del cache_dict[int(n)-tnl] # no need to retain the files which have already been read \n",
    "        \n",
    "        if np.random.rand() < 0.0005 : \n",
    "            sprint (hit, miss, len(cache_dict))\n",
    "            sprint (prediction.shape, y.shape, dist.shape, len(count_CS))\n",
    "    if case in [\"mean\", \"ratios\", \"max\", \"min\", \"fractional\"]:\n",
    "        return (count_CS)\n",
    "    elif case in [\"default\"]:\n",
    "        return len(count_CS)\n",
    "\n",
    "\n",
    "def determine_y_thresh_by_maximising_variance_around_mean(max_dist, N, method):\n",
    "    std = {}\n",
    "    mean = {}\n",
    "    count = 1 \n",
    "    for i in tqdm(np.arange(0, max_dist, abs(0-max_dist)/20), desc=\"Finding y_thresh\"):\n",
    "        l = compute_criticality_smooth(model.predict, \"training_data_8_4_8\", y_thresh=i, \\\n",
    "                                       tnl=8, N=N, case=method, PM=False)\n",
    "        std[i] = np.std(l)\n",
    "        mean[i] = np.mean(l)\n",
    "\n",
    "        count += 1 \n",
    "    return (std), mean\n",
    "    \n",
    "\n",
    "for method in [\"default\", \"fractional\"]:\n",
    "    for i in [10, 20, 50, 100, 200, 400]:\n",
    "        var_dict, mean_dict = determine_y_thresh_by_maximising_variance_around_mean(max_dist=3500, \\\n",
    "                                                                                    N = i, method = method) \n",
    "        # sprint (var_dict)\n",
    "        # plt.show()\n",
    "        plt.plot( list(var_dict.keys()), list(var_dict.values()) , label=\"var@\" + str(i) + \"points\",\\\n",
    "                 color=\"blue\",alpha=i/400)\n",
    "        plt.plot( list(mean_dict.keys()), list(mean_dict.values()) , label=\"mean@\" + str(i) + \" points\", \\\n",
    "                 color=\"red\", alpha=i/400)\n",
    "    plt.legend(fontsize=6)\n",
    "    plt.xlabel(r\"y_thresh\")\n",
    "    plt.ylabel(r\"Complexity metric \"+ method)\n",
    "    plt.show()\n",
    "\n",
    "# sprint ()\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bd901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorflow.keras.backend.clear_session()\n",
    "x = np.random.rand(16, 8, 32, 32, 1)\n",
    "inp = layers.Input(shape=(None, *x.shape[2:]))\n",
    "\n",
    "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
    "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(5, 5),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inp)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation=\"relu\", padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# Next, we will build the complete model and compile it.\n",
    "model = tensorflow.keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=tensorflow.keras.losses.binary_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import os \n",
    "import glob \n",
    "from tensorflow.keras.losses import MeanAbsoluteError, BinaryCrossentropy\n",
    "from smartprint import smartprint as sprint\n",
    "\n",
    "# Create CSVLogger callback with specified filename\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ComputeMetrics(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        criticality = compute_criticality_smooth(self.model.predict, \"training_data_8_4_8\", y_thresh=1500, \\\n",
    "                                       tnl=8, N=200, case=\"fractional\", PM=False)\n",
    "        sprint (np.mean(criticality))\n",
    "        logs['CSR_train_data_smooth'] = np.mean(criticality)\n",
    "    \n",
    "class CustomDataGenerator(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, data_dir, num_samples, batch_size=32, shuffle=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(1, num_samples+1)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in indexes:\n",
    "            file_x = os.path.join(self.data_dir, '{}_x.npy'.format(i))\n",
    "            file_y = os.path.join(self.data_dir, '{}_y.npy'.format(i))\n",
    "            x = np.load(file_x)\n",
    "            y = np.load(file_y)\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "        x_batch = np.array(x_batch)\n",
    "        y_batch = np.array(y_batch)\n",
    "#         x_batch = np.swapaxes(x_batch, 0, -1)\n",
    "#         y_batch = np.swapaxes(y_batch, 0, -1)\n",
    "        x_batch = np.moveaxis(x_batch, [0, 1, 2, 3], [0, 2, 3, 1])\n",
    "        y_batch = np.moveaxis(y_batch, [0, 1, 2, 3], [0, 2, 3, 1])    \n",
    "\n",
    "#         sprint (x_batch[..., np.newaxis].shape, (y_batch[..., np.newaxis]).shape)\n",
    "        return (x_batch[..., np.newaxis]), (y_batch[..., np.newaxis])\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f5bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69286eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    global M\n",
    "    squared_difference = tf.square((y_true)*M - (y_pred)*M)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  \n",
    "\n",
    "def non_zero_mape(y_true, y_pred):\n",
    "    t = y_true[y_true> 10]\n",
    "    p = y_pred[y_true> 10]\n",
    "    \n",
    "    squared_difference = tf.abs((t-p)/t)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  \n",
    "\n",
    "for n_depth in range(1):# [1,2,3,4,5,6]:\n",
    "\n",
    "\n",
    "    # Generate some random training data\n",
    "#     input_shape = (32, 32, 3)\n",
    "    output_channels = 1\n",
    "#     x_train = np.random.rand(num_samples, *input_shape)\n",
    "#     y_train = np.random.rand(num_samples, input_shape[0], input_shape[1], output_channels)\n",
    "\n",
    "    # Create the model\n",
    "    input_shape = (32, 32, 8)\n",
    "\n",
    "#     model = FFNN(2, 1, 32, 32, 8, 1)\n",
    "#     model = CNN(3, 16, 32, 32, 8, 8, kernel_size=3)\n",
    "#     model = custom_unet(\n",
    "#         input_shape=(32, 32, 8),\n",
    "#         use_batch_norm=True,\n",
    "#         num_classes=1,\n",
    "#         filters=16,\n",
    "#         dropout=0,\n",
    "#         output_activation='relu')\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # Compile the model with a mean squared error loss and an Adam optimizer\n",
    "#     loss_fn = my_loss_fn # losses.MeanSquaredError()\n",
    "#     loss_fn = \"mae\"\n",
    "    \n",
    "    loss_fn = \"mse\"\n",
    "    optimizer = optimizers.Adam(1e-3)\n",
    "#     optimizer = optimizers.RMSprop()\n",
    "#     optimizer = \"sgd\"\n",
    "#     metrics = metrics.MeanAbsoluteError()\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=non_zero_mape)\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='auto')\n",
    "\n",
    "#     # Train the model with early stopping\n",
    "#     history = model.fit(x_train, y_train, validation_data = (x_train, y_train),\\\n",
    "#                         batch_size=batch_size, epochs=epochs, callbacks=[earlystop])\n",
    "\n",
    "\n",
    "    # Evaluate the model on some validation data\n",
    "#     x_val = np.random.rand(num_samples, *input_shape)\n",
    "#     y_val = np.random.rand(num_samples, input_shape[0], input_shape[1], output_channels)\n",
    "#     val_loss, val_mae = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "#     print(f\"Validation loss: {val_loss:.4f}, validation mean absolute error: {val_mae:.4f}\")\n",
    "\n",
    "\n",
    "#     from tensorflow.keras.utils import plot_model\n",
    "#     model = unet(num_layers=n_depth, base_filters=64, input_shape=(256, 256, 3), num_classes=1, batch_norm=True)\n",
    "#     plot_model(model, to_file=str(n_depth) + 'unet.png', show_shapes=False, show_layer_names=False) # , show_layer_activations=False)\n",
    "\n",
    "           \n",
    "    train_data_folder = \"training_data_8_4_8\"\n",
    "    validation_data_folder = \"validation_data_8_4_8\"\n",
    "\n",
    "    num_train = len(glob.glob(train_data_folder + \"/*_x.npy\")) \n",
    "    num_validation = len(glob.glob(validation_data_folder + \"/*_x.npy\")) \n",
    "\n",
    "    \n",
    "    r = 0.1 # np.random.rand()\n",
    "    train_gen = CustomDataGenerator(data_dir=train_data_folder, \\\n",
    "                                    num_samples=int(num_train*r), batch_size=batch_size, shuffle=True)\n",
    "    validation_gen = CustomDataGenerator(data_dir=validation_data_folder, \\\n",
    "                                         num_samples=int(num_validation*r), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "        \n",
    "    filename = str(n_depth) + '_validation_loss.csv'\n",
    "\n",
    "    csv_logger = CSVLogger(filename)\n",
    "    tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=\"log_dir\")\n",
    "\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='auto')\n",
    "    model.fit(train_gen, validation_data=validation_gen, epochs=epochs, callbacks=[earlystop, csv_logger,\\\n",
    "                                                                                   tensorboard_callback, \\\n",
    "                                                                                   ComputeMetrics()],\\\n",
    "                                                                             workers=6)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log_dir\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776d779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b38e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6028ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartprint import smartprint as sprint \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "for x,y in train_gen:\n",
    "    print (x.shape, y.shape)\n",
    "    print (np.max(x[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee53cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295bd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imshowpair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imshowpair\n",
    "for k in range(16):\n",
    "    for x,y in validation_gen:\n",
    "        print (x.shape, y.shape)\n",
    "        print (np.max(x[0]))\n",
    "        break\n",
    "\n",
    "#     plt.imshow(y[k, 0, :, :, 0])\n",
    "#     plt.colorbar()\n",
    "\n",
    "\n",
    "    sprint (x.shape, y.shape)\n",
    "\n",
    "    yp = model.predict(x)\n",
    "#     sprint (y.shape)\n",
    "\n",
    "#     plt.imshow(y[k, 0, :, :, 0])\n",
    "#     plt.colorbar()\n",
    "    imshowpair.imshowpair(y[k, 0, :, :, 0], yp[k, 1, :, :, 0])\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d12a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea317872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bd01b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_required_to_converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9961c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}